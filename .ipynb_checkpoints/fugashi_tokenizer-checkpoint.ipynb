{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629a5fd1-0377-4884-a908-31eb9e433295",
   "metadata": {},
   "source": [
    "# Fugashi with Unidic-Lite Tokenizer-Dictionary System\n",
    "\n",
    "**started 11/20/2024**\n",
    "\n",
    "website link: https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html\n",
    "\n",
    "**Bibtext Citation (just double click on this to get the correct formatting for putting in a LaTeX document)**\n",
    "\n",
    "@inproceedings{mccann-2020-fugashi,\n",
    "    title = \"fugashi, a Tool for Tokenizing {J}apanese in Python\",\n",
    "    author = \"McCann, Paul\",\n",
    "    booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\",\n",
    "    month = nov,\n",
    "    year = \"2020\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/2020.nlposs-1.7\",\n",
    "    pages = \"44--51\",\n",
    "    abstract = \"Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack English documentation. This paper introduces fugashi, a MeCab wrapper for Python, and gives an introduction to tokenizing Japanese.\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "_Alicia Roberts Fall 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d911699-d5c9-42d8-8018-98bb8fe92061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tokenizer: \n",
    "\n",
    "# !pip install fugashi[unidic-lite] \n",
    "\n",
    "# note this can take a long time to download \n",
    "\n",
    "# might benefit from installing locally for future runs of this notebook\n",
    "# just run this in your command prompt connected to your path to install\n",
    "# (assuming pip is also installed on your path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab74c06-6336-42eb-99cc-dc1fe310eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccff9b2a-66fa-47ed-8a50-33249cd23d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger \"holds state about the dictionary\" \n",
    "# which I think just means it is our currently used dictionary \n",
    "# if we choose to change later\n",
    "\n",
    "tagger = fugashi.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41be82a6-a457-4ce6-a194-6c1637c431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49976013-217b-4295-9118-b7f8c230ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真 夜中 の ドア を たたき 。 帰ら ない で と 泣い た 。 あの 季節 が 今 目 の 前\n"
     ]
    }
   ],
   "source": [
    "words = [word.surface for word in tagger(sample_text)]\n",
    "print(*words)    # just \"print(words)\" returns the list, \n",
    "                 #but adding * returns the sentence with the spaces the tokenizer added \n",
    "                 # (each space denotes a new token has been made)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf7108-d09e-49b6-ae7b-ab4d027e04aa",
   "metadata": {},
   "source": [
    "_Notice how it doesn't split every hiragana character into its own token, I am unsure how it would react with words written in katakana, so lets see how it does on カエル　for frog. It seems to be able to tell what is a conjugation and what is a particle, so that's good!_\n",
    "　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec7790c1-c807-4add-8d77-cbb69749f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カエル は あ その リンゴ を 食べ たい 。\n"
     ]
    }
   ],
   "source": [
    "frog = \"カエルはあそのリンゴを食べたい。\"\n",
    "print(*[word.surface for word in tagger(frog)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739c04b-3656-4815-8471-2935bf054029",
   "metadata": {},
   "source": [
    "_cool liking it so far_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62738635-a8e9-4c4d-85e7-528290b0a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 二 世 も - 一 一 世 ^^ 心 せ ょ \" 米 國 鄉軍 は 顔 る 公平 ね 0 -\n"
     ]
    }
   ],
   "source": [
    "# now let's see how it does on one of our sample texts:\n",
    "\n",
    "sample = '1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-'\n",
    "print(*[word.surface for word in tagger(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b64d8-32ac-4c9c-a0b2-3813cba5226f",
   "metadata": {},
   "source": [
    "_so It doesn't break when wrong chacters are added, but it also split up issei and nisei, so I might have to modify the dictionary to count that as a word_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7741ccf-6fb6-433f-9b0c-30f293b88574",
   "metadata": {},
   "source": [
    "## Using Lemma to avoid ambiguitity \n",
    "\n",
    "So since Japanese has many words with the same meaning, this tokenizer has the ability to return the lemma of a word (say if it is written in hiragana, it will try to interpret its meaning and return its kanji version so there is little ambiguity at its meaning\n",
    "\n",
    "_take なく ー＞ 鳴く as an example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0c2aff2-b84d-42d4-9e46-15743eb21db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし の ははは せ が たかい です 。\n"
     ]
    }
   ],
   "source": [
    "# example: \n",
    "\n",
    "# my mother is very tall, all written in hiragana: \n",
    "\n",
    "hiragana_text = 'わたしのはははせがたかいです。'\n",
    "\n",
    "print(*[word.surface for word in tagger(hiragana_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07fc75bd-da6e-4e60-9cb9-4f9aea413f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし\t私\n",
      "の\tの\n",
      "ははは\tははは\n",
      "せ\t背\n",
      "が\tが\n",
      "たかい\t高い\n",
      "です\tです\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "# see that it didn't split はは　from は\n",
    "\n",
    "# now taking the lemma: \n",
    "\n",
    "for word in tagger(hiragana_text):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6ea0a5a-2987-46fb-aa48-22f724df74fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし\t私\n",
      "の\tの\n",
      "お\t御\n",
      "かあ\t母\n",
      "さん\tさん\n",
      "は\tは\n",
      "せ\t背\n",
      "が\tが\n",
      "たかい\t高い\n",
      "です\tです\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "# i bet it would do better with okaasan!\n",
    "\n",
    "hiragana_text2 = 'わたしのおかあさんはせがたかいです。'\n",
    "\n",
    "for word in tagger(hiragana_text2):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1809d-cd5b-4bbf-9a83-45a5016b6bda",
   "metadata": {},
   "source": [
    "_see that even though no one is practically going to write the lemma for the honorific お using its lemma reduces ambiguity from it being something else or it being given the same meaning as another お that shows up in the same text or even in another text when we beginng training sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da6241c8-476c-47d8-833e-f85bcc65e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食べ\t食べる\n",
      "食べ\t食べる\n",
      "たい\tたい\n",
      "食べ\t食べる\n",
      "ます\tます\n",
      "食べ\t食べる\n",
      "なく\tない\n",
      "て\tて\n",
      "食べ\t食べる\n",
      "ない\tない\n",
      "たべ\t食べる\n",
      "た\tた\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "verb_string = \"食べ食べたい食べます食べなくて食べないたべた。\"\n",
    "#testing how it choosen lemma for the same verb but different conjugations\n",
    "\n",
    "for word in tagger(verb_string):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7441dbf-f05c-4c1f-a158-4cdb8dd4e207",
   "metadata": {},
   "source": [
    "_okay so far I'm satisfied with this result, it is splitting the lemma correctly and keeping the part of the conjugation that adds context, such as wanting to do something or if its past tense, etc. . ._\n",
    "\n",
    "_this means that one word is being split into multiple tokens, were the inflection is being separate from the stem: example in english being changing looked = look + ed which makes sense, look is important to the meaning, and ed is implied to be past tense, same thing for たべた＝食べる＋た_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3ac28-ac74-49a2-bfb6-f33aab7b7684",
   "metadata": {},
   "source": [
    "## Computing Power\n",
    "\n",
    "It takes a lot for the computer to run tagging, so vectorize when you can. This is very easy to do when using data frames like that of pandas, so shoudln't be difficult to implement.\n",
    "\n",
    "Creating a new tagger is much more expensive than just using the same tagger in a list comprehension or a vectorized or for loop approach\n",
    "\n",
    "But basically just don't reasign tagger, just use the same one you define in the beginning as \"tagger\" instead of fugashi.Tagger()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fcd90-7518-441d-b639-c5ba5c48d3af",
   "metadata": {},
   "source": [
    "## Testing it on a Sample Data set\n",
    "\n",
    "Given my small pre-data set for training this model, let's see how it does on splitting up the strings of yes and nos.\n",
    "\n",
    "Will it be able to keep issei as one word or will it be split up into ichi + sei? \n",
    "\n",
    "My _hope_ is that it will be able to distinguish from context when it is a generational term or just gibberish, which can be tested a lot of different ways, but let's see how this method goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e27e680-6aed-4ad0-8918-4f813f1ad462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article link</th>\n",
       "      <th>Date</th>\n",
       "      <th>classification</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/n...</td>\n",
       "      <td>1940/02/16</td>\n",
       "      <td>1</td>\n",
       "      <td>會员大募集運動市協活動準備第一世諸氏の援助協力を希望</td>\n",
       "      <td>seems good to me, is using it as a generationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/06</td>\n",
       "      <td>1</td>\n",
       "      <td>一世行進曲 | ’ ，， 常石芝靑作</td>\n",
       "      <td>needs to be verified, but seems related to poe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>-1</td>\n",
       "      <td>1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-</td>\n",
       "      <td>OCR read 二 as 一一 resulting in 二世 looking like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>しズ 0 t 家 * に纖されねぱな -^ a* 今や 19 始時代から永らく奮 H を續け...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>此第一世の遺</td>\n",
       "      <td>\"this first generation's legacy\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>故に一世 1 二世备 &lt; も在</td>\n",
       "      <td>translation is VERY wrong: 1 is supposed to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>大統領遺舉に付き第一世に訴ふ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Daijūkyūseiki Shinbun 1893.02.04: Page 2</td>\n",
       "      <td>1893/02/04</td>\n",
       "      <td>0</td>\n",
       "      <td>二世界</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sōkō Hyōron 1893.12.03: Page 14</td>\n",
       "      <td>1893/12/03</td>\n",
       "      <td>0</td>\n",
       "      <td>一人</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shin Sekai 1895.08.28: Page 1</td>\n",
       "      <td>1895/08/28</td>\n",
       "      <td>-1</td>\n",
       "      <td>一世に雄飛せるのみ</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shin Sekai 1895.11.05: Page 1</td>\n",
       "      <td>1895/11/05</td>\n",
       "      <td>0</td>\n",
       "      <td>(Title 一)世界</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chōsen Shinpō 1896.12.18: Page 2</td>\n",
       "      <td>1896/12/18</td>\n",
       "      <td>0</td>\n",
       "      <td>(panel divider) 世</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yamato Shinbun 1899.01.04: Page 2</td>\n",
       "      <td>1899/01/04</td>\n",
       "      <td>-1</td>\n",
       "      <td>一世紀</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Yamato Shinbun 1899.01.10: Page 5</td>\n",
       "      <td>1899/01/10</td>\n",
       "      <td>0</td>\n",
       "      <td>故に世間</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Yamato Shinbun 1899.01.26: Page 1</td>\n",
       "      <td>1899/01/26</td>\n",
       "      <td>0</td>\n",
       "      <td>一番</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article link        Date  \\\n",
       "0   https://hojishinbun.hoover.org/en/newspapers/n...  1940/02/16   \n",
       "1   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/06   \n",
       "2   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "3   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "4   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "5   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "6   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "7            Daijūkyūseiki Shinbun 1893.02.04: Page 2  1893/02/04   \n",
       "8                     Sōkō Hyōron 1893.12.03: Page 14  1893/12/03   \n",
       "9                       Shin Sekai 1895.08.28: Page 1  1895/08/28   \n",
       "10                      Shin Sekai 1895.11.05: Page 1  1895/11/05   \n",
       "11                   Chōsen Shinpō 1896.12.18: Page 2  1896/12/18   \n",
       "12                  Yamato Shinbun 1899.01.04: Page 2  1899/01/04   \n",
       "13                  Yamato Shinbun 1899.01.10: Page 5  1899/01/10   \n",
       "14                  Yamato Shinbun 1899.01.26: Page 1  1899/01/26   \n",
       "\n",
       "    classification                                               text  \\\n",
       "0                1                         會员大募集運動市協活動準備第一世諸氏の援助協力を希望   \n",
       "1                1                                 一世行進曲 | ’ ，， 常石芝靑作   \n",
       "2               -1                 1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-   \n",
       "3                1  しズ 0 t 家 * に纖されねぱな -^ a* 今や 19 始時代から永らく奮 H を續け...   \n",
       "4                1                                             此第一世の遺   \n",
       "5                1                                    故に一世 1 二世备 < も在   \n",
       "6                1                                     大統領遺舉に付き第一世に訴ふ   \n",
       "7                0                                                二世界   \n",
       "8                0                                                 一人   \n",
       "9               -1                                          一世に雄飛せるのみ   \n",
       "10               0                                        (Title 一)世界   \n",
       "11               0                                  (panel divider) 世   \n",
       "12              -1                                                一世紀   \n",
       "13               0                                               故に世間   \n",
       "14               0                                                 一番   \n",
       "\n",
       "                                             comments  \n",
       "0   seems good to me, is using it as a generationa...  \n",
       "1   needs to be verified, but seems related to poe...  \n",
       "2   OCR read 二 as 一一 resulting in 二世 looking like ...  \n",
       "3                                                 NaN  \n",
       "4                    \"this first generation's legacy\"  \n",
       "5   translation is VERY wrong: 1 is supposed to be...  \n",
       "6                                                 NaN  \n",
       "7                these are from other students' work!  \n",
       "8                these are from other students' work!  \n",
       "9                these are from other students' work!  \n",
       "10               these are from other students' work!  \n",
       "11               these are from other students' work!  \n",
       "12               these are from other students' work!  \n",
       "13               these are from other students' work!  \n",
       "14               these are from other students' work!  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries and data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('issei_training_data - Sheet1.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f25a7e1-8681-46f2-866d-37a352c2cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s, tagger):\n",
    "    '''given a string S from a text, a sample, or whatever, split it up into its tokens using Fugashi's TAGGER'''\n",
    "    words = [word.surface for word in tagger(s)]\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ac2613c-8dbe-441a-9a6f-95adbdbda3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 一世 in str?: False \n",
      "str:  ( Title 一 ) 世界 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  1 二 世 も - 一 一 世 ^^ 心 せ ょ \" 米 國 鄉軍 は 顔 る 公平 ね 0 - \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  此 第 一 世 の 遺 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  故 に 世間 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  一人 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  此 第 一 世 の 遺 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  此 第 一 世 の 遺 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  ( panel divider ) 世 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  二 世界 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = 3 # just to run this a few times\n",
    "\n",
    "for i in np.arange(iterations): \n",
    "    samples = data.sample(3)['text'].values # take a random sample of 5 data points from our data set\n",
    "    splits = [split_text(s, tagger) for s in samples] # split each sample into its tokens based on our tagger\n",
    "    for l in range(len(splits)):\n",
    "        print('Is 一世 in str?:', '一世' in splits[l], '\\nstr: ', *splits[l], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2dda98-95d8-45c0-8863-6bc3cbb94ad3",
   "metadata": {},
   "source": [
    "So it seems that issei is not in this dictionary, at least not in any way I can tell, let's see if we can investigate this further\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8a4ceff-5c23-40e3-92b1-1731dabb87c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m           Tagger\n",
       "\u001b[1;31mString form:\u001b[0m    <fugashi.fugashi.Tagger object at 0x0000013EEEDC3340>\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\alica\\anaconda3\\lib\\site-packages\\fugashi\\fugashi.cp39-win_amd64.pyd\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Default tagger. Detects the correct Unidic feature format.\n",
       "\n",
       "Unidic 2.1.2 (17 field) and 2.2, 2.3 format (29 field) are supported.\n",
       "\u001b[1;31mCall docstring:\u001b[0m Wrapper for parseToNodeList.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagger?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d91a8-ff93-466f-8166-a02f9d6b58f5",
   "metadata": {},
   "source": [
    "The documentation (https://pypi.org/project/fugashi/) says that you can use any dictionary you want, so I might have investigate into dictionaries that have issei in them, or manually add it in myself to an existing copy of a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd892be4-9c4e-4eab-b96d-36fbad8c47eb",
   "metadata": {},
   "source": [
    "## next steps: \n",
    "\n",
    "1. explore OCR and furher applications \n",
    "2. narrow down the methods I want to do \n",
    "3. implement better tokenization and find more documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fd6be-4d9f-4671-b9d9-26defe78202a",
   "metadata": {},
   "source": [
    "## Implementation idea: remove issei and nisei, then tokenize. \n",
    "\n",
    "what it fixes: since issei and nisei are not being recognized as compound words, if we know that each string in our training set will contain nissei and issei, and we can check future tests for the presence, we can just remove the word from the string and see if we can get context from the removal of this. this means that we shouldn't lose context from the surrounding characters if the use of issei and nisei is correct (ie, not a mis-translationg on the OCR's part) \n",
    "\n",
    "what issues it might cause: \n",
    "if it is _not_ a hit, and instead is a mistranslation or picking up on neighboring words that share characters, then we are losing information that is important and can confuse the model. \n",
    "\n",
    "\n",
    "all in all, I think it's worth trying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006ce43-465d-4314-95c0-1e024cf021ba",
   "metadata": {},
   "source": [
    "**with the small data set we are working with:**\n",
    "\n",
    "let's try vectorizing using a one hot encoding method and use PCA to remove computational power and the cost of our model. \n",
    "\n",
    "**steps**\n",
    "\n",
    "\n",
    "1. remove the hit (eitheer issei, nisei, or any other word you want to find the useage of) from the string. \n",
    "> Don't change the original string. You can store this in a new data frame or add a column to the imported data set. \n",
    "> also record _where_ in the string the hit occurs so you can focus your tokenizing in that area. Assume that context that can help determine our word usage decreases as the characters get further away from the hit. \n",
    "\n",
    "2. Tokenize the string\n",
    ">this can be done using whatever tokenizer you want, this notebook is using the fugashi tokenizer. \n",
    "> I will also be returning the _lema_ so that there is a reduction in the ambiguity of words that have the same hiragana spelling being treated the same when they mean different things (take hana as flower va hana as nose) \n",
    "> the fugashi tokenizer does a fairly good job picking up on the kanji meaning from context, but this could also be an area of error to account for.  \n",
    "\n",
    "3. clean the data to only include the neighboring characters/words, so pick a size (say, 20 characters on either side of the hit) to reduce the storage cost of these training points. \n",
    "> if there are multiple hits, split the hit into multiple data points. This should already be implemented in the csv file itself, but just incase run a scan to see if there are multiple hits in one data point. \n",
    ">Be sure that overlap won't matter _don't change the original data, only a copy of it_ \n",
    "\n",
    "\n",
    "4. remove stop words\n",
    "> these are words that don't add value, such as OCR mishaps (characters like |, ^, /, { that show up in the transcription) and particles like を、が、は、で, etc. . .\n",
    "> you can also choose to remove all but the stems of verbs if you don't care about the conjugations and only the presence of the verb. \n",
    "\n",
    "5. extend the string (which is now an array) to where each value gets its own column. This is done through pivoting the table and having a count be the values in the table. \n",
    ">This is what i refer to as One Hot Encoding. This is where we assign either 1 or 0 to a characteristic, where in this case we are saying the existence of a word in our string is the characteristic. This can be very costly, as we can have MANY different words in all of our stirngs. \n",
    "\n",
    "6. PCA - Principle Component Analysis \n",
    "> this determines which characteristsics have the greatest affect on the classification of our target (ie, is issei/ nisei being used the way we want it to be used to be a hit?) by measuring the variance of all strings that have this word in it. \n",
    "> we then choose the top N characteristics that give us the most variance (just variation in our options) so that we can have a trained model. Think that if every string has the same word in it, this would be varaince 0. \n",
    ">We can make no distinction between if the use of our target word is one meaning or another if they all have the same word, so this word would be tossed out of our analysis to reduce computing power and also make the model run faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9caa3dc-0b79-4838-872d-16ca60feb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample string to use for testing the functions \n",
    "\n",
    "sample_text = '真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16017c9a-c28f-44ec-8b9a-49bdf77f3628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING without HIT:  真夜中のドアをたたき。帰らないでと泣いた。あのが今目の前\n",
      "index of first character of HIT in STRING:  23\n",
      "the hit based on remove_hit index: 季節\n"
     ]
    }
   ],
   "source": [
    "# step 1: removing the hit\n",
    "\n",
    "def remove_hit(string, hit):\n",
    "    '''return a shortened version of STRING that is centered around HIT with N characters on each side of it\n",
    "    STRING: any string\n",
    "    HIT: any word\n",
    "    returns:\n",
    "    STRING wihout HIT, N: any positive integer that is the location of HIT in STRING. will return the first occurence of the first character of HIT'''\n",
    "    n = -1 # this means there is no occurence of HIT \n",
    "    mod_string = ''\n",
    "    if (hit in string): # first see that HIT is actually in STRING to avoid errors\n",
    "        size_hit = len(hit) # how many characters to examine at once \n",
    "        for n in range(len(string) - size_hit): # itterate through STRING until you reach HIT\n",
    "            if string[n:n+size_hit] == hit: # iterating till we reach HIT\n",
    "                mod_string = string[0:n] + string[n+size_hit:] # create a modified string without HIT \n",
    "                return mod_string, n   # return modified string + index value of the first occurence \n",
    "    \n",
    "    # if HIT is not in STRING, raise an error:\n",
    "    raise LookupError(hit +' not in STRING')\n",
    "\n",
    "    \n",
    "    \n",
    "# test case:\n",
    "hit = '季節'\n",
    "print('STRING without HIT: ',remove_hit(sample_text, hit)[0])\n",
    "print('index of first character of HIT in STRING: ',remove_hit(sample_text, hit)[1])\n",
    "\n",
    "print('the hit based on remove_hit index:',sample_text[23:23 +len(hit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1af5440e-f87b-430b-9a1b-9acbf5e03a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真 夜中 の ドア-door を 叩き 。 返る ない て と 泣く た 。 彼の 季節 が 今 目 の 前\n"
     ]
    }
   ],
   "source": [
    "# step 2: tokenizing \n",
    "\n",
    "cleaned_string, n_hit = remove_hit(sample_text, hit)\n",
    "\n",
    "\n",
    "tokens = [word.feature.lemma for word in tagger(sample_text)]\n",
    "print(*tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858709a9-8737-4cfe-af68-43241ad231bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
