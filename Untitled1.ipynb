{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629a5fd1-0377-4884-a908-31eb9e433295",
   "metadata": {},
   "source": [
    "# Fugashi with Unidic-Lite Tokenizer-Dictionary System\n",
    "\n",
    "**started 11/20/2024**\n",
    "\n",
    "website link: https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d911699-d5c9-42d8-8018-98bb8fe92061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fugashi[unidic-lite]\n",
      "  Downloading fugashi-1.4.0-cp39-cp39-win_amd64.whl (512 kB)\n",
      "     -------------------------------------- 512.6/512.6 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting unidic-lite\n",
      "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
      "     -------------------------------------- 47.4/47.4 MB 788.5 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: unidic-lite\n",
      "  Building wheel for unidic-lite (setup.py): started\n",
      "  Building wheel for unidic-lite (setup.py): finished with status 'done'\n",
      "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=cf67886cc1c203d08c6cfedbcbe5646cf4aa6b395378e7bf26dd5e503366d3d6\n",
      "  Stored in directory: c:\\users\\alica\\appdata\\local\\pip\\cache\\wheels\\56\\9c\\4f\\2c115e896b4b6c584039ca19de3581d333856782ef108cdc5c\n",
      "Successfully built unidic-lite\n",
      "Installing collected packages: unidic-lite, fugashi\n",
      "Successfully installed fugashi-1.4.0 unidic-lite-1.0.8\n"
     ]
    }
   ],
   "source": [
    "# installing tokenizer: \n",
    "\n",
    "!pip install fugashi[unidic-lite]\n",
    "\n",
    "# note this can take a long time to download \n",
    "\n",
    "# might benefit from installing locally for future runs of this notebook\n",
    "# just run this in your command prompt connected to your path to install\n",
    "# (assuming pip is also installed on your path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab74c06-6336-42eb-99cc-dc1fe310eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccff9b2a-66fa-47ed-8a50-33249cd23d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger \"holds state about the dictionary\" \n",
    "# which I think just means it is our currently used dictionary \n",
    "# if we choose to change later\n",
    "\n",
    "tagger = fugashi.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41be82a6-a457-4ce6-a194-6c1637c431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49976013-217b-4295-9118-b7f8c230ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真 夜中 の ドア を たたき 。 帰ら ない で と 泣い た 。 あの 季節 が 今 目 の 前\n"
     ]
    }
   ],
   "source": [
    "words = [word.surface for word in tagger(sample_text)]\n",
    "print(*words)    # just \"print(words)\" returns the list, \n",
    "                 #but adding * returns the sentence with the spaces the tokenizer added \n",
    "                 # (each space denotes a new token has been made)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf7108-d09e-49b6-ae7b-ab4d027e04aa",
   "metadata": {},
   "source": [
    "_Notice how it doesn't split every hiragana character into its own token, I am unsure how it would react with words written in katakana, so lets see how it does on カエル　for frog. It seems to be able to tell what is a conjugation and what is a particle, so that's good!_\n",
    "　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec7790c1-c807-4add-8d77-cbb69749f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カエル は あ その リンゴ を 食べ たい 。\n"
     ]
    }
   ],
   "source": [
    "frog = \"カエルはあそのリンゴを食べたい。\"\n",
    "print(*[word.surface for word in tagger(frog)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739c04b-3656-4815-8471-2935bf054029",
   "metadata": {},
   "source": [
    "_cool liking it so far_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62738635-a8e9-4c4d-85e7-528290b0a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 二 世 も - 一 一 世 ^^ 心 せ ょ \" 米 國 鄉軍 は 顔 る 公平 ね 0 -\n"
     ]
    }
   ],
   "source": [
    "# now let's see how it does on one of our sample texts:\n",
    "\n",
    "sample = '1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-'\n",
    "print(*[word.surface for word in tagger(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b64d8-32ac-4c9c-a0b2-3813cba5226f",
   "metadata": {},
   "source": [
    "_so It doesn't break when wrong chacters are added, but it also split up issei and nisei, so I might have to modify the dictionary to count that as a word_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7741ccf-6fb6-433f-9b0c-30f293b88574",
   "metadata": {},
   "source": [
    "## Using Lemma to avoid ambiguitity \n",
    "\n",
    "So since Japanese has many words with the same meaning, this tokenizer has the ability to return the lemma of a word (say if it is written in hiragana, it will try to interpret its meaning and return its kanji version so there is little ambiguity at its meaning\n",
    "\n",
    "_take なく ー＞ 鳴く as an example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0c2aff2-b84d-42d4-9e46-15743eb21db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし の ははは せ が たかい です 。\n"
     ]
    }
   ],
   "source": [
    "# example: \n",
    "\n",
    "# my mother is very tall, all written in hiragana: \n",
    "\n",
    "hiragana_text = 'わたしのはははせがたかいです。'\n",
    "\n",
    "print(*[word.surface for word in tagger(hiragana_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07fc75bd-da6e-4e60-9cb9-4f9aea413f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし\t私\n",
      "の\tの\n",
      "ははは\tははは\n",
      "せ\t背\n",
      "が\tが\n",
      "たかい\t高い\n",
      "です\tです\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "# see that it didn't split はは　from は\n",
    "\n",
    "# now taking the lemma: \n",
    "\n",
    "for word in tagger(hiragana_text):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6ea0a5a-2987-46fb-aa48-22f724df74fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし\t私\n",
      "の\tの\n",
      "お\t御\n",
      "かあ\t母\n",
      "さん\tさん\n",
      "は\tは\n",
      "せ\t背\n",
      "が\tが\n",
      "たかい\t高い\n",
      "です\tです\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "# i bet it would do better with okaasan!\n",
    "\n",
    "hiragana_text2 = 'わたしのおかあさんはせがたかいです。'\n",
    "\n",
    "for word in tagger(hiragana_text2):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1809d-cd5b-4bbf-9a83-45a5016b6bda",
   "metadata": {},
   "source": [
    "_see that even though no one is practically going to write the lemma for the honorific お using its lemma reduces ambiguity from it being something else or it being given the same meaning as another お that shows up in the same text or even in another text when we beginng training sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da6241c8-476c-47d8-833e-f85bcc65e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食べ\t食べる\n",
      "食べ\t食べる\n",
      "たい\tたい\n",
      "食べ\t食べる\n",
      "ます\tます\n",
      "食べ\t食べる\n",
      "なく\tない\n",
      "て\tて\n",
      "食べ\t食べる\n",
      "ない\tない\n",
      "たべ\t食べる\n",
      "た\tた\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "verb_string = \"食べ食べたい食べます食べなくて食べないたべた。\"\n",
    "#testing how it choosen lemma for the same verb but different conjugations\n",
    "\n",
    "for word in tagger(verb_string):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7441dbf-f05c-4c1f-a158-4cdb8dd4e207",
   "metadata": {},
   "source": [
    "_okay so far I'm satisfied with this result, it is splitting the lemma correctly and keeping the part of the conjugation that adds context, such as wanting to do something or if its past tense, etc. . ._\n",
    "\n",
    "_this means that one word is being split into multiple tokens, were the inflection is being separate from the stem: example in english being changing looked = look + ed which makes sense, look is important to the meaning, and ed is implied to be past tense, same thing for たべた＝食べる＋た_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3ac28-ac74-49a2-bfb6-f33aab7b7684",
   "metadata": {},
   "source": [
    "## Computing Power\n",
    "\n",
    "It takes a lot for the computer to run tagging, so vectorize when you can. This is very easy to do when using data frames like that of pandas, so shoudln't be difficult to implement.\n",
    "\n",
    "Creating a new tagger is much more expensive than just using the same tagger in a list comprehension or a vectorized or for loop approach\n",
    "\n",
    "But basically just don't reasign tagger, just use the same one you define in the beginning as \"tagger\" instead of fugashi.Tagger()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efeae3-843f-42c8-aef4-a5f99d7ad970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
