{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629a5fd1-0377-4884-a908-31eb9e433295",
   "metadata": {},
   "source": [
    "# Fugashi with Unidic-Lite Tokenizer-Dictionary System\n",
    "\n",
    "**started 11/20/2024**\n",
    "\n",
    "website link: https://www.dampfkraft.com/nlp/how-to-tokenize-japanese.html\n",
    "\n",
    "**Bibtext Citation (just double click on this to get the correct formatting for putting in a LaTeX document)**\n",
    "\n",
    "@inproceedings{mccann-2020-fugashi,\n",
    "    title = \"fugashi, a Tool for Tokenizing {J}apanese in Python\",\n",
    "    author = \"McCann, Paul\",\n",
    "    booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\",\n",
    "    month = nov,\n",
    "    year = \"2020\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/2020.nlposs-1.7\",\n",
    "    pages = \"44--51\",\n",
    "    abstract = \"Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack English documentation. This paper introduces fugashi, a MeCab wrapper for Python, and gives an introduction to tokenizing Japanese.\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "_Alicia Roberts Fall 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7d911699-d5c9-42d8-8018-98bb8fe92061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tokenizer: \n",
    "\n",
    "# !pip install fugashi[unidic-lite] \n",
    "\n",
    "# note this can take a long time to download \n",
    "\n",
    "# might benefit from installing locally for future runs of this notebook\n",
    "# just run this in your command prompt connected to your path to install\n",
    "# (assuming pip is also installed on your path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eab74c06-6336-42eb-99cc-dc1fe310eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries:\n",
    "\n",
    "import fugashi # tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccff9b2a-66fa-47ed-8a50-33249cd23d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger \"holds state about the dictionary\" \n",
    "# which I think just means it is our currently used dictionary \n",
    "# if we choose to change later\n",
    "\n",
    "tagger = fugashi.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41be82a6-a457-4ce6-a194-6c1637c431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49976013-217b-4295-9118-b7f8c230ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真 夜中 の ドア を たたき 。 帰ら ない で と 泣い た 。 あの 季節 が 今 目 の 前\n"
     ]
    }
   ],
   "source": [
    "words = [word.surface for word in tagger(sample_text)]\n",
    "print(*words)    # just \"print(words)\" returns the list, \n",
    "                 #but adding * returns the sentence with the spaces the tokenizer added \n",
    "                 # (each space denotes a new token has been made)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf7108-d09e-49b6-ae7b-ab4d027e04aa",
   "metadata": {},
   "source": [
    "_Notice how it doesn't split every hiragana character into its own token, I am unsure how it would react with words written in katakana, so lets see how it does on カエル　for frog. It seems to be able to tell what is a conjugation and what is a particle, so that's good!_\n",
    "　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ec7790c1-c807-4add-8d77-cbb69749f366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カエル は あ その リンゴ を 食べ たい 。\n"
     ]
    }
   ],
   "source": [
    "frog = \"カエルはあそのリンゴを食べたい。\"\n",
    "print(*[word.surface for word in tagger(frog)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739c04b-3656-4815-8471-2935bf054029",
   "metadata": {},
   "source": [
    "_cool liking it so far_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62738635-a8e9-4c4d-85e7-528290b0a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 二 世 も - 一 一 世 ^^ 心 せ ょ \" 米 國 鄉軍 は 顔 る 公平 ね 0 -\n"
     ]
    }
   ],
   "source": [
    "# now let's see how it does on one of our sample texts:\n",
    "\n",
    "sample = '1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-'\n",
    "print(*[word.surface for word in tagger(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b64d8-32ac-4c9c-a0b2-3813cba5226f",
   "metadata": {},
   "source": [
    "_so It doesn't break when wrong chacters are added, but it also split up issei and nisei, so I might have to modify the dictionary to count that as a word_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7741ccf-6fb6-433f-9b0c-30f293b88574",
   "metadata": {},
   "source": [
    "## Using Lemma to avoid ambiguitity \n",
    "\n",
    "So since Japanese has many words with the same meaning, this tokenizer has the ability to return the lemma of a word (say if it is written in hiragana, it will try to interpret its meaning and return its kanji version so there is little ambiguity at its meaning\n",
    "\n",
    "_take なく ー＞ 鳴く as an example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c0c2aff2-b84d-42d4-9e46-15743eb21db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし の ははは せ が たかい です 。\n"
     ]
    }
   ],
   "source": [
    "# example: \n",
    "\n",
    "# my mother is very tall, all written in hiragana: \n",
    "\n",
    "hiragana_text = 'わたしのはははせがたかいです。'\n",
    "\n",
    "print(*[word.surface for word in tagger(hiragana_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "07fc75bd-da6e-4e60-9cb9-4f9aea413f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし\t私\n",
      "の\tの\n",
      "ははは\tははは\n",
      "せ\t背\n",
      "が\tが\n",
      "たかい\t高い\n",
      "です\tです\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "# see that it didn't split はは　from は\n",
    "\n",
    "# now taking the lemma: \n",
    "\n",
    "for word in tagger(hiragana_text):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6ea0a5a-2987-46fb-aa48-22f724df74fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "わたし\t私\n",
      "の\tの\n",
      "お\t御\n",
      "かあ\t母\n",
      "さん\tさん\n",
      "は\tは\n",
      "せ\t背\n",
      "が\tが\n",
      "たかい\t高い\n",
      "です\tです\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "# i bet it would do better with okaasan!\n",
    "\n",
    "hiragana_text2 = 'わたしのおかあさんはせがたかいです。'\n",
    "\n",
    "for word in tagger(hiragana_text2):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1809d-cd5b-4bbf-9a83-45a5016b6bda",
   "metadata": {},
   "source": [
    "_see that even though no one is practically going to write the lemma for the honorific お using its lemma reduces ambiguity from it being something else or it being given the same meaning as another お that shows up in the same text or even in another text when we beginng training sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da6241c8-476c-47d8-833e-f85bcc65e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食べ\t食べる\n",
      "食べ\t食べる\n",
      "たい\tたい\n",
      "食べ\t食べる\n",
      "ます\tます\n",
      "食べ\t食べる\n",
      "なく\tない\n",
      "て\tて\n",
      "食べ\t食べる\n",
      "ない\tない\n",
      "たべ\t食べる\n",
      "た\tた\n",
      "。\t。\n"
     ]
    }
   ],
   "source": [
    "verb_string = \"食べ食べたい食べます食べなくて食べないたべた。\"\n",
    "#testing how it choosen lemma for the same verb but different conjugations\n",
    "\n",
    "for word in tagger(verb_string):\n",
    "    print(word.surface, word.feature.lemma, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7441dbf-f05c-4c1f-a158-4cdb8dd4e207",
   "metadata": {},
   "source": [
    "_okay so far I'm satisfied with this result, it is splitting the lemma correctly and keeping the part of the conjugation that adds context, such as wanting to do something or if its past tense, etc. . ._\n",
    "\n",
    "_this means that one word is being split into multiple tokens, were the inflection is being separate from the stem: example in english being changing looked = look + ed which makes sense, look is important to the meaning, and ed is implied to be past tense, same thing for たべた＝食べる＋た_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3ac28-ac74-49a2-bfb6-f33aab7b7684",
   "metadata": {},
   "source": [
    "## Computing Power\n",
    "\n",
    "It takes a lot for the computer to run tagging, so vectorize when you can. This is very easy to do when using data frames like that of pandas, so shoudln't be difficult to implement.\n",
    "\n",
    "Creating a new tagger is much more expensive than just using the same tagger in a list comprehension or a vectorized or for loop approach\n",
    "\n",
    "But basically just don't reasign tagger, just use the same one you define in the beginning as \"tagger\" instead of fugashi.Tagger()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fcd90-7518-441d-b639-c5ba5c48d3af",
   "metadata": {},
   "source": [
    "## Testing it on a Sample Data set\n",
    "\n",
    "Given my small pre-data set for training this model, let's see how it does on splitting up the strings of yes and nos.\n",
    "\n",
    "Will it be able to keep issei as one word or will it be split up into ichi + sei? \n",
    "\n",
    "My _hope_ is that it will be able to distinguish from context when it is a generational term or just gibberish, which can be tested a lot of different ways, but let's see how this method goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e27e680-6aed-4ad0-8918-4f813f1ad462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article link</th>\n",
       "      <th>Date</th>\n",
       "      <th>classification</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/n...</td>\n",
       "      <td>1940/02/16</td>\n",
       "      <td>1</td>\n",
       "      <td>會员大募集運動市協活動準備第一世諸氏の援助協力を希望</td>\n",
       "      <td>seems good to me, is using it as a generationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/06</td>\n",
       "      <td>1</td>\n",
       "      <td>一世行進曲 | ’ ，， 常石芝靑作</td>\n",
       "      <td>needs to be verified, but seems related to poe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>-1</td>\n",
       "      <td>1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-</td>\n",
       "      <td>OCR read 二 as 一一 resulting in 二世 looking like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>しズ 0 t 家 * に纖されねぱな -^ a* 今や 19 始時代から永らく奮 H を續け...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>此第一世の遺</td>\n",
       "      <td>\"this first generation's legacy\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>故に一世 1 二世备 &lt; も在</td>\n",
       "      <td>translation is VERY wrong: 1 is supposed to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://hojishinbun.hoover.org/en/newspapers/k...</td>\n",
       "      <td>1940/10/18</td>\n",
       "      <td>1</td>\n",
       "      <td>大統領遺舉に付き第一世に訴ふ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Daijūkyūseiki Shinbun 1893.02.04: Page 2</td>\n",
       "      <td>1893/02/04</td>\n",
       "      <td>0</td>\n",
       "      <td>二世界</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sōkō Hyōron 1893.12.03: Page 14</td>\n",
       "      <td>1893/12/03</td>\n",
       "      <td>0</td>\n",
       "      <td>一人</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shin Sekai 1895.08.28: Page 1</td>\n",
       "      <td>1895/08/28</td>\n",
       "      <td>-1</td>\n",
       "      <td>一世に雄飛せるのみ</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shin Sekai 1895.11.05: Page 1</td>\n",
       "      <td>1895/11/05</td>\n",
       "      <td>0</td>\n",
       "      <td>(Title 一)世界</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chōsen Shinpō 1896.12.18: Page 2</td>\n",
       "      <td>1896/12/18</td>\n",
       "      <td>0</td>\n",
       "      <td>(panel divider) 世</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yamato Shinbun 1899.01.04: Page 2</td>\n",
       "      <td>1899/01/04</td>\n",
       "      <td>-1</td>\n",
       "      <td>一世紀</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Yamato Shinbun 1899.01.10: Page 5</td>\n",
       "      <td>1899/01/10</td>\n",
       "      <td>0</td>\n",
       "      <td>故に世間</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Yamato Shinbun 1899.01.26: Page 1</td>\n",
       "      <td>1899/01/26</td>\n",
       "      <td>0</td>\n",
       "      <td>一番</td>\n",
       "      <td>these are from other students' work!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article link        Date  \\\n",
       "0   https://hojishinbun.hoover.org/en/newspapers/n...  1940/02/16   \n",
       "1   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/06   \n",
       "2   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "3   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "4   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "5   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "6   https://hojishinbun.hoover.org/en/newspapers/k...  1940/10/18   \n",
       "7            Daijūkyūseiki Shinbun 1893.02.04: Page 2  1893/02/04   \n",
       "8                     Sōkō Hyōron 1893.12.03: Page 14  1893/12/03   \n",
       "9                       Shin Sekai 1895.08.28: Page 1  1895/08/28   \n",
       "10                      Shin Sekai 1895.11.05: Page 1  1895/11/05   \n",
       "11                   Chōsen Shinpō 1896.12.18: Page 2  1896/12/18   \n",
       "12                  Yamato Shinbun 1899.01.04: Page 2  1899/01/04   \n",
       "13                  Yamato Shinbun 1899.01.10: Page 5  1899/01/10   \n",
       "14                  Yamato Shinbun 1899.01.26: Page 1  1899/01/26   \n",
       "\n",
       "    classification                                               text  \\\n",
       "0                1                         會员大募集運動市協活動準備第一世諸氏の援助協力を希望   \n",
       "1                1                                 一世行進曲 | ’ ，， 常石芝靑作   \n",
       "2               -1                 1 二世も - 一一世 ^^ 心せょ \" 米國鄉軍は顔る公平ね 0-   \n",
       "3                1  しズ 0 t 家 * に纖されねぱな -^ a* 今や 19 始時代から永らく奮 H を續け...   \n",
       "4                1                                             此第一世の遺   \n",
       "5                1                                    故に一世 1 二世备 < も在   \n",
       "6                1                                     大統領遺舉に付き第一世に訴ふ   \n",
       "7                0                                                二世界   \n",
       "8                0                                                 一人   \n",
       "9               -1                                          一世に雄飛せるのみ   \n",
       "10               0                                        (Title 一)世界   \n",
       "11               0                                  (panel divider) 世   \n",
       "12              -1                                                一世紀   \n",
       "13               0                                               故に世間   \n",
       "14               0                                                 一番   \n",
       "\n",
       "                                             comments  \n",
       "0   seems good to me, is using it as a generationa...  \n",
       "1   needs to be verified, but seems related to poe...  \n",
       "2   OCR read 二 as 一一 resulting in 二世 looking like ...  \n",
       "3                                                 NaN  \n",
       "4                    \"this first generation's legacy\"  \n",
       "5   translation is VERY wrong: 1 is supposed to be...  \n",
       "6                                                 NaN  \n",
       "7                these are from other students' work!  \n",
       "8                these are from other students' work!  \n",
       "9                these are from other students' work!  \n",
       "10               these are from other students' work!  \n",
       "11               these are from other students' work!  \n",
       "12               these are from other students' work!  \n",
       "13               these are from other students' work!  \n",
       "14               these are from other students' work!  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries and data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('issei_training_data - Sheet1.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f25a7e1-8681-46f2-866d-37a352c2cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s, tagger):\n",
    "    '''given a string S from a text, a sample, or whatever, split it up into its tokens using Fugashi's TAGGER'''\n",
    "    words = [word.surface for word in tagger(s)]\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ac2613c-8dbe-441a-9a6f-95adbdbda3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 一世 in str?: False \n",
      "str:  一 世 に 雄飛 せる のみ \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  一人 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  大統領 遺 舉 に 付き 第 一 世 に 訴 ふ \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  會 员大 募集 運動 市 協 活動 準備 第 一 世 諸氏 の 援助 協力 を 希望 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  大統領 遺 舉 に 付き 第 一 世 に 訴 ふ \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  一 世紀 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  大統領 遺 舉 に 付き 第 一 世 に 訴 ふ \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  一人 \n",
      "\n",
      "Is 一世 in str?: False \n",
      "str:  一番 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = 3 # just to run this a few times\n",
    "\n",
    "for i in np.arange(iterations): \n",
    "    samples = data.sample(3)['text'].values # take a random sample of 5 data points from our data set\n",
    "    splits = [split_text(s, tagger) for s in samples] # split each sample into its tokens based on our tagger\n",
    "    for l in range(len(splits)):\n",
    "        print('Is 一世 in str?:', '一世' in splits[l], '\\nstr: ', *splits[l], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2dda98-95d8-45c0-8863-6bc3cbb94ad3",
   "metadata": {},
   "source": [
    "So it seems that issei is not in this dictionary, at least not in any way I can tell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e8a4ceff-5c23-40e3-92b1-1731dabb87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d91a8-ff93-466f-8166-a02f9d6b58f5",
   "metadata": {},
   "source": [
    "The documentation (https://pypi.org/project/fugashi/) says that you can use any dictionary you want, so I might have investigate into dictionaries that have issei in them, or manually add it in myself to an existing copy of a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd892be4-9c4e-4eab-b96d-36fbad8c47eb",
   "metadata": {},
   "source": [
    "## next steps: \n",
    "\n",
    "1. explore OCR and furher applications \n",
    "2. narrow down the methods I want to do \n",
    "3. implement better tokenization and find more documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fd6be-4d9f-4671-b9d9-26defe78202a",
   "metadata": {},
   "source": [
    "## Implementation idea: remove issei and nisei, then tokenize. \n",
    "\n",
    "what it fixes: since issei and nisei are not being recognized as compound words, if we know that each string in our training set will contain nissei and issei, and we can check future tests for the presence, we can just remove the word from the string and see if we can get context from the removal of this. this means that we shouldn't lose context from the surrounding characters if the use of issei and nisei is correct (ie, not a mis-translationg on the OCR's part) \n",
    "\n",
    "what issues it might cause: \n",
    "if it is _not_ a hit, and instead is a mistranslation or picking up on neighboring words that share characters, then we are losing information that is important and can confuse the model. \n",
    "\n",
    "\n",
    "all in all, I think it's worth trying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006ce43-465d-4314-95c0-1e024cf021ba",
   "metadata": {},
   "source": [
    "**with the small data set we are working with:**\n",
    "\n",
    "let's try vectorizing using a one hot encoding method and use PCA to remove computational power and the cost of our model. \n",
    "\n",
    "**steps**\n",
    "\n",
    "\n",
    "1. remove the hit (eitheer issei, nisei, or any other word you want to find the useage of) from the string. \n",
    "> Don't change the original string. You can store this in a new data frame or add a column to the imported data set. \n",
    "> also record _where_ in the string the hit occurs so you can focus your tokenizing in that area. Assume that context that can help determine our word usage decreases as the characters get further away from the hit. \n",
    "\n",
    "2. Tokenize the string\n",
    ">this can be done using whatever tokenizer you want, this notebook is using the fugashi tokenizer. \n",
    "> I will also be returning the _lema_ so that there is a reduction in the ambiguity of words that have the same hiragana spelling being treated the same when they mean different things (take hana as flower va hana as nose) \n",
    "> the fugashi tokenizer does a fairly good job picking up on the kanji meaning from context, but this could also be an area of error to account for.  \n",
    "\n",
    "3. remove stop words\n",
    "> these are words that don't add value, such as OCR mishaps (characters like |, ^, /, { that show up in the transcription) and particles like を、が、は、で, etc. . .\n",
    "> you can also choose to remove all but the stems of verbs if you don't care about the conjugations and only the presence of the verb. \n",
    " \n",
    "\n",
    "\n",
    "4. clean the data to only include the neighboring characters/words, so pick a size (say, 20 characters on either side of the hit) to reduce the storage cost of these training points. \n",
    "> if there are multiple hits, split the hit into multiple data points. This should already be implemented in the csv file itself, but just incase run a scan to see if there are multiple hits in one data point. \n",
    ">Be sure that overlap won't matter _don't change the original data, only a copy of it_\n",
    "\n",
    "5. extend the string (which is now an array) to where each value gets its own column. This is done through pivoting the table and having a count be the values in the table. \n",
    ">This is what i refer to as One Hot Encoding. This is where we assign either 1 or 0 to a characteristic, where in this case we are saying the existence of a word in our string is the characteristic. This can be very costly, as we can have MANY different words in all of our stirngs. \n",
    "\n",
    "6. PCA - Principle Component Analysis \n",
    "> this determines which characteristsics have the greatest affect on the classification of our target (ie, is issei/ nisei being used the way we want it to be used to be a hit?) by measuring the variance of all strings that have this word in it. \n",
    "> we then choose the top N characteristics that give us the most variance (just variation in our options) so that we can have a trained model. Think that if every string has the same word in it, this would be varaince 0. \n",
    ">We can make no distinction between if the use of our target word is one meaning or another if they all have the same word, so this word would be tossed out of our analysis to reduce computing power and also make the model run faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e9caa3dc-0b79-4838-872d-16ca60feb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample string to use for testing the functions \n",
    "\n",
    "sample_text = '真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf28b2-bef1-4ee7-a15c-7318adb38ea9",
   "metadata": {},
   "source": [
    "### Step 1: removing the hit from the string\n",
    "\n",
    "To do this, I wrote a function remove_hit, which just removes the hit from the string and returns the index of the first character of the first hit. If there are multiple, it will return one string, but multiple index values. If there is no hit, it will return an empty string and n = -1. This will be useful for cleaning in the future. You can just filter out all rows that have n = -1 so they are not used in the training or tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "16017c9a-c28f-44ec-8b9a-49bdf77f3628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING without HIT:  真夜中のドアをたたき。帰らないでと泣いた。あのが今目の前\n",
      "index of first character of HIT in STRING:  23\n",
      "the hit based on remove_hit index: 季節\n",
      "\n",
      "STRING without HIT:  おはよう、。どこですか？\n",
      "index of first character of HIT in STRING:  [5, 12]\n",
      "the hits based on remove_hit index: お母さん お母さん\n",
      "\n",
      "STRING without HIT:  \n",
      "index of first character of HIT in STRING:  -1\n",
      "the hit based on remove_hit index: \n"
     ]
    }
   ],
   "source": [
    "# step 1: removing the hit\n",
    "\n",
    "def remove_hit(string, hit):\n",
    "    '''return a shortened version of STRING that is centered around HIT with N characters on each side of it\n",
    "    STRING: any string\n",
    "    HIT: any word\n",
    "    returns:\n",
    "    STRING wihout HIT, N: any positive integer that is the location of HIT in STRING. will return the first occurence of the first character of HIT'''\n",
    "    ns = [] # this means there is no occurence of HIT if empty\n",
    "    mod_string = ''\n",
    "    \n",
    "    if (hit in string): # first see that HIT is actually in STRING to avoid errors\n",
    "        size_hit = len(hit) # how many characters to examine at once \n",
    "        \n",
    "        for n in range(len(string) - size_hit): # itterate through STRING until you reach HIT\n",
    "            \n",
    "            if string[n:n+size_hit] == hit: # iterating till we reach HIT\n",
    "                if mod_string == '':\n",
    "                    mod_string = string[0:n] + string[n+size_hit:] # create a modified string without HIT\n",
    "                    \n",
    "                else:\n",
    "                    mod_string = mod_string[0:n - size_hit] + mod_string[n:] # since mod_string is already 3 indeces shorter, you have to account for that \n",
    "                ns.append(n)    \n",
    "        if len(ns) == 1:\n",
    "            return np.array(mod_string), ns[0]# return modified string + index value of the first occurence \n",
    "        return np.array(mod_string), ns\n",
    "    \n",
    "    # if HIT is not in STRING, return empty string and -1 (to be removed later):\n",
    "    return '', -1 #\n",
    "\n",
    "    \n",
    "    \n",
    "# test case:\n",
    "hit = '季節'\n",
    "print('STRING without HIT: ',remove_hit(sample_text, hit)[0])\n",
    "print('index of first character of HIT in STRING: ',remove_hit(sample_text, hit)[1])\n",
    "\n",
    "print('the hit based on remove_hit index:',sample_text[23:23 +len(hit)])\n",
    "\n",
    "# second test: multiple hits: \n",
    "\n",
    "multiple_hits = 'おはよう、お母さん。どこお母さんですか？'\n",
    "\n",
    "hit = 'お母さん'\n",
    "\n",
    "print('\\nSTRING without HIT: ',remove_hit(multiple_hits, hit)[0])\n",
    "print('index of first character of HIT in STRING: ',remove_hit(multiple_hits, hit)[1])\n",
    "print('the hits based on remove_hit index:',multiple_hits[5:5 +len(hit)], multiple_hits[12:12+len(hit)] )\n",
    "\n",
    "\n",
    "# third test: no hit:\n",
    "\n",
    "hit = '山田'\n",
    "print('\\nSTRING without HIT: ',remove_hit(sample_text, hit)[0])\n",
    "print('index of first character of HIT in STRING: ',remove_hit(sample_text, hit)[1])\n",
    "print('the hit based on remove_hit index:',sample_text[-1:-1 + len(hit)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da67833-2d9b-4f36-b6d9-1c08490df3f2",
   "metadata": {},
   "source": [
    "### Step 2: Tokenizing \n",
    "\n",
    "use the function TOKENIZE to take  STRING and turn it into its tokens. The tokens will be the lemma form, so they might not resemble the original string, but this is to avoid ambiguity when training with synonyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1af5440e-f87b-430b-9a1b-9acbf5e03a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\t   真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前 \n",
      "tokenized version: 真 夜中 の ドア-door を 叩き 。 返る ない て と 泣く た 。 彼の 季節 が 今 目 の 前\n"
     ]
    }
   ],
   "source": [
    "# step 2: tokenizing \n",
    "\n",
    "cleaned_string, n_hit = remove_hit(sample_text, hit)\n",
    "\n",
    "def tokenize(string):\n",
    "    '''given a string STRING, return the tokens in lemma form in the form of a numpy array'''\n",
    "    tokens = np.array([word.feature.lemma for word in tagger(string)]) # store the lemma of each token\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# test case: \n",
    "\n",
    "sample_tokens = tokenize(sample_text)\n",
    "\n",
    "print('original text:\\t  ' ,sample_text, '\\ntokenized version:',  *sample_tokens) # looks good! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb902e-611a-4791-b741-c8cba036049c",
   "metadata": {},
   "source": [
    "### Step 3: Remove Stop Words \n",
    "\n",
    "Using a list of common japanese stop words I got from this githib repo: https://github.com/stopwords-iso/stopwords-ja/tree/master, I created a list of stop words in combination with anticipated ones such as mistranslated characters and puncutation. There could verywell be more I haven't anticipated, which can just be added to the end of ADD_WORDS array in the next cell.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "This assumes the strings are in JAPANESE, so any latin based characters will NOT be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "858709a9-8737-4cfe-af68-43241ad231bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: remove extra characters and stop words:\n",
    "\n",
    "# read in stop words library: \n",
    "\n",
    "stop_words_data = pd.read_csv('stopwords-ja.txt', sep = ' ', header = None)\n",
    "\n",
    "# I think we need to convert these to LEMMA first -- so they are actually removed if it is a stop word \n",
    "\n",
    "# adding new ones that I anticipate seeing: \n",
    "\n",
    "add_words = ['、','。','・','!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '+', '=', 'I', '{', '}', '[', ']', '0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "stop_words = np.append(stop_words_data.values, add_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "54eceaa7-3941-4d44-8c28-dda42327e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's remove stop words: \n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    '''given a list of tokens TOKENS, remove all stop words'''\n",
    "    new_tokens = [] # to store non-stop words \n",
    "    for t in tokens: \n",
    "        if t not in stop_words: \n",
    "            new_tokens.append(t)\n",
    "    return np.array(new_tokens) # to keep as an np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ab6c7c21-044a-49a7-a09c-cd5b0003e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list of tokens ['真' '夜中' 'の' 'ドア-door' 'を' '叩き' '。' '返る' 'ない' 'て' 'と' '泣く' 'た' '。' '彼の'\n",
      " '季節' 'が' '今' '目' 'の' '前']\n",
      "\n",
      "New cleaned list of tokens: ['真' '夜中' 'ドア-door' '叩き' '返る' '泣く' '彼の' '季節' '今' '目' '前']\n"
     ]
    }
   ],
   "source": [
    "print('Original list of tokens', sample_tokens)\n",
    "print('\\nNew cleaned list of tokens:', remove_stopwords(sample_tokens))\n",
    "\n",
    "\n",
    "sample_tokens_cleaned = remove_stopwords(sample_tokens)\n",
    "# note this removes verb stems, so if you want to keep verb stems, you would have to remove that from the list. I recommend doing this BEFORE turning the dataframe into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb1874-83b2-4c4b-8b14-97160b983b0d",
   "metadata": {},
   "source": [
    "### Step 4: Clean Data by Shortening Strings \n",
    "\n",
    "This is for long arrays, or just long strings of tokens. This requires using the index location from the remove_hits function to identify where to center your shortened string. For this method, the actual position of the characters do not matter, but if this were to be extended into a BERT NN, then the order would be preserved. \n",
    "\n",
    "Choose a value N that determine the number of characters to keep on either side of the hit index. If the value of N extends beyond the length of the array, then the value of N will be reduced on the side that exceeds the range. This means that if you choose N = a million or something, you should get the original array returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2ecaf851-5310-418c-a63f-ed303329ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_tokens(tokens, index, d = len(tokens)//2): # oh theres an error here \n",
    "                                         # -- we remove the words that show the index, so we need to change n as we go \n",
    "                                         #-- i'll cut it in half for now\n",
    "    '''given an array of TOKENS, return the array as a shortened version centered at INDEX where there are D characters on either side of the index'''\n",
    "    return tokens[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e984dac4-8b44-4d20-a1e4-3f359b8a0293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真夜中のドアをたたき。帰らないでと泣いた。あの季節が今目の前\n",
      "泣く\n"
     ]
    }
   ],
   "source": [
    "hit = '季節'\n",
    "index = remove_hit(sample_text, hit)[1]\n",
    "\n",
    "print(sample_text)\n",
    "print(shorten_tokens(sample_tokens_cleaned, index//4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8ae9154c-f785-402c-83bf-4d686e8da495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd5ab3-3101-4ff0-9bdf-fa4efac3e628",
   "metadata": {},
   "source": [
    "_This step still needs work, but I think we can make our table we want first to get things rolling_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce42b75-4ecd-4955-b32a-32f186327b76",
   "metadata": {},
   "source": [
    "### Step 5: make the training table!\n",
    "\n",
    "To do this, have all data samples with their tokens as a column and then pivot the table so the column values are the new column labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "03d3fc81-c58f-4a71-8443-0962a7abe812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['真', '夜中', 'ドア-door', '叩き', '返る', '泣く', '彼の', '季節', '今', '目', '前'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokens_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8824923e-0fc2-4f32-9d0f-4fa64368a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = pd.DataFrame(data = {'test':sample_tokens_cleaned})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "23f55b2d-f51f-4632-b01e-db9e51a02300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>真</td>\n",
       "      <td>夜中</td>\n",
       "      <td>ドア-door</td>\n",
       "      <td>叩き</td>\n",
       "      <td>返る</td>\n",
       "      <td>泣く</td>\n",
       "      <td>彼の</td>\n",
       "      <td>季節</td>\n",
       "      <td>今</td>\n",
       "      <td>目</td>\n",
       "      <td>前</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1        2   3   4   5   6   7  8  9  10\n",
       "test  真  夜中  ドア-door  叩き  返る  泣く  彼の  季節  今  目  前"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1466e100-520f-4d98-8423-97254d4a5b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>真</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>夜中</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ドア-door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>叩き</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>返る</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>泣く</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>彼の</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>季節</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>今</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>前</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test\n",
       "0         真\n",
       "1        夜中\n",
       "2   ドア-door\n",
       "3        叩き\n",
       "4        返る\n",
       "5        泣く\n",
       "6        彼の\n",
       "7        季節\n",
       "8         今\n",
       "9         目\n",
       "10        前"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91623cd-6cd3-45e5-8326-34ca28347de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
